{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "edge_prediction_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZCzE0JS2c3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download the Charades dataset\n",
        "!wget http://ai2-website.s3.amazonaws.com/data/Charades_v1_480.zip\n",
        "\n",
        "#Unzip the file\n",
        "!unzip /content/Charades_v1_480.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM-VnNJj22kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Do the one frame rate image extraction from videos\n",
        "#The following videos are chosen as training and testing data\n",
        "#The action class 'Putting a bag somewhere' is extracted\n",
        "#Output frames should then be sent into video scene graph pipeline to get the results in a pickle file\n",
        "\n",
        "!mv /content/Charades_v1_480/F4BJJ.mp4 /content\n",
        "!mkdir /content/F4BJJ\n",
        "!ffmpeg -ss 7.70 -t 5.00 -accurate_seek -i F4BJJ.mp4 -r 1 {'/content/F4BJJ/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/T1CQE.mp4 /content\n",
        "#!mkdir /content/T1CQE\n",
        "#!ffmpeg -ss 17.30 -t 13.00 -accurate_seek -i T1CQE.mp4 -r 1 {'/content/T1CQE/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/1D31Z.mp4 /content\n",
        "#!mkdir /content/1D31Z\n",
        "#!ffmpeg -ss 10.40 -t 7.00 -accurate_seek -i 1D31Z.mp4 -r 1 {'/content/1D31Z/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/MIV2M.mp4 /content\n",
        "#!mkdir /content/MIV2M\n",
        "#!ffmpeg -ss 6.40 -t 5.00 -accurate_seek -i MIV2M.mp4 -r 1 {'/content/MIV2M/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/GKH4A.mp4 /content\n",
        "#!mkdir /content/GKH4A\n",
        "#!ffmpeg -ss 21.50 -t 7.00 -accurate_seek -i GKH4A.mp4 -r 1 {'/content/GKH4A/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/SXYLN.mp4 /content\n",
        "#!mkdir /content/SXYLN\n",
        "#!ffmpeg -ss 0.20 -t 6.00 -accurate_seek -i SXYLN.mp4 -r 1 {'/content/SXYLN/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/EDKXT.mp4 /content\n",
        "#!mkdir /content/EDKXT\n",
        "#!ffmpeg -ss 3.50 -t 8.00 -accurate_seek -i EDKXT.mp4 -r 1 {'/content/EDKXT/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/FM5D5.mp4 /content\n",
        "#!mkdir /content/FM5D5\n",
        "#!ffmpeg -ss 11.30 -t 6.00 -accurate_seek -i FM5D5.mp4 -r 1 {'/content/FM5D5/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/1SCZE.mp4 /content\n",
        "#!mkdir /content/1SCZE\n",
        "#!ffmpeg -ss 8.00 -t 4.00 -accurate_seek -i 1SCZE.mp4 -r 1 {'/content/1SCZE/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/ZWPS3.mp4 /content\n",
        "#!mkdir /content/ZWPS3\n",
        "#!ffmpeg -ss 12.50 -t 5.00 -accurate_seek -i ZWPS3.mp4 -r 1 {'/content/ZWPS3/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/ENFL6.mp4 /content\n",
        "#!mkdir /content/ENFL6\n",
        "#!ffmpeg -ss 3.90 -t 15.00 -accurate_seek -i ENFL6.mp4 -r 1 {'/content/ENFL6/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/L0YQM.mp4 /content\n",
        "#!mkdir /content/L0YQM\n",
        "#!ffmpeg -ss 15.60 -t 6.00 -accurate_seek -i L0YQM.mp4 -r 1 {'/content/L0YQM/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/3VM0R.mp4 /content\n",
        "#!mkdir /content/3VM0R\n",
        "#!ffmpeg -ss 20.70 -t 7.00 -accurate_seek -i 3VM0R.mp4 -r 1 {'/content/3VM0R/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/0TM53.mp4 /content\n",
        "#!mkdir /content/0TM53\n",
        "#!ffmpeg -ss 5.80 -t 8.00 -accurate_seek -i 0TM53.mp4 -r 1 {'/content/0TM53/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/CDULZ.mp4 /content\n",
        "#!mkdir /content/CDULZ\n",
        "#!ffmpeg -ss 15.90 -t 14.00 -accurate_seek -i CDULZ.mp4 -r 1 {'/content/CDULZ/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/1FX8Q.mp4 /content\n",
        "#!mkdir /content/1FX8Q\n",
        "#!ffmpeg -ss 1.70 -t 5.00 -accurate_seek -i 1FX8Q.mp4 -r 1 {'/content/1FX8Q/'}%d.jpg\n",
        "\n",
        "#!mv /content/Charades_v1_480/SM4AO.mp4 /content\n",
        "#!mkdir /content/SM4AO\n",
        "#!ffmpeg -ss 1.00 -t 9.00 -accurate_seek -i SM4AO.mp4 -r 1 {'/content/SM4AO/'}%d.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n_jWDE73p-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the pickle file generated from the video scene graph pipeline\n",
        "import pickle\n",
        "with open('/content/kern_sgdet.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn8MGuX_4d9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate the human-object scene graph from the pickle file\n",
        "#The scene graph has top 10 human-object edges\n",
        "#In this example, the seventh image in the pickle file is chosen\n",
        "last_graph = data[6]\n",
        "last_man_rel = []\n",
        "max_man_rel = []\n",
        "for i in range (len(last_graph['pred_classes'])):\n",
        "  if(last_graph['pred_classes'][i] == 78):\n",
        "    first_man = i\n",
        "    break\n",
        "for i in range (len(last_graph['rel_scores'])):\n",
        "  if(last_graph['pred_rel_inds'][i][0] == first_man):\n",
        "    other_scores = last_graph['rel_scores'][i]\n",
        "    other_scores[0] = 0\n",
        "    node_index = last_graph['pred_rel_inds'][i][1]\n",
        "    node = last_graph['pred_classes'][node_index]\n",
        "    rel_index = np.argmax(other_scores)\n",
        "    max_val = np.array(other_scores).max()\n",
        "    box = last_graph['pred_boxes'][node_index]\n",
        "    max_man_rel.append((max_val, node, rel_index, box))\n",
        "max_man_rel.sort(reverse=True)\n",
        "for i in range (10):\n",
        "  class_index = max_man_rel[i][1]\n",
        "  rel_index = max_man_rel[i][2]\n",
        "  jso_1 = jso['idx_to_label'][str(class_index)]\n",
        "  jso_2 = jso['idx_to_predicate'][str(rel_index)]\n",
        "  box = max_man_rel[i][3]\n",
        "  last_man_rel.append((class_index, jso_1, rel_index, jso_2, box))\n",
        "print(last_man_rel)\n",
        "\n",
        "#The scene graph is orgnaised into one array\n",
        "man_box = last_graph['pred_boxes'][first_man]\n",
        "inputs = np.append(78, man_box)\n",
        "for i in range (10):\n",
        "  class_in = last_man_rel[i][0]\n",
        "  box_in = last_man_rel[i][4]\n",
        "  new_input = np.append(class_in, box_in)\n",
        "  inputs = np.append(inputs, new_input, axis=0)\n",
        "print(inputs)\n",
        "\n",
        "#Resize it for 11 nodes\n",
        "inputs.resize((11, 5))\n",
        "print(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTOUUrtv510d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Install libraries\n",
        "!pip install dgl\n",
        "!pip install torch\n",
        "\n",
        "#Build a new graph\n",
        "node_count = 11\n",
        "edge_list = []\n",
        "for i in range (1, 11):\n",
        "  edge_list.append((0, i))\n",
        "\n",
        "#Use the networkx library to visualise the graph, this step can be skipped\n",
        "import dgl\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "def build_karate_club_graph(edges):\n",
        "    g = dgl.DGLGraph()\n",
        "    g.add_nodes(node_count)\n",
        "    src, dst = tuple(zip(*edges))\n",
        "    g.add_edges(src, dst)\n",
        "    return g\n",
        "\n",
        "G = build_karate_club_graph(edge_list)\n",
        "# Since the actual graph is undirected, we convert it for visualization purpose.\n",
        "nx_G = G.to_networkx()\n",
        "# Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n",
        "pos = nx.kamada_kawai_layout(nx_G)\n",
        "nx.draw(nx_G, pos, with_labels=True, node_color=[[.7, .7, .7]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9vwKo8a60ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GCN Layer is built\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the message and reduce function\n",
        "# NOTE: We ignore the GCN's normalization constant c_ij for this tutorial.\n",
        "def gcn_message(edges):\n",
        "    # The argument is a batch of edges.\n",
        "    # This computes a (batch of) message called 'msg' using the source node's feature 'h'.\n",
        "    return {'msg' : edges.src['h']}\n",
        "\n",
        "def gcn_reduce(nodes):\n",
        "    # The argument is a batch of nodes.\n",
        "    # This computes the new 'h' features by summing received 'msg' in each node's mailbox.\n",
        "    return {'h' : torch.sum(nodes.mailbox['msg'], dim=1)}\n",
        "\n",
        "# Define the GCNLayer module\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.linear = nn.Linear(in_feats, out_feats)\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        # g is the graph and the inputs is the input node features\n",
        "        # first set the node features\n",
        "        g.ndata['h'] = inputs\n",
        "        # trigger message passing on all edges\n",
        "        g.send(g.edges(), gcn_message)\n",
        "        # trigger aggregation at all nodes\n",
        "        g.recv(g.nodes(), gcn_reduce)\n",
        "        # get the result node features\n",
        "        h = g.ndata.pop('h')\n",
        "        # perform linear transformation\n",
        "        return self.linear(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUoar6We68yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The Modle is built\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_size, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gcn1 = GCNLayer(in_feats, hidden_size)\n",
        "        #self.gcn3 = GCNLayer(hidden_size, hidden_size)\n",
        "        #self.gcn4 = GCNLayer(hidden_size, hidden_size)\n",
        "        self.gcn2 = GCNLayer(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = self.gcn1(g, inputs)\n",
        "        h = torch.relu(h)\n",
        "        #h = self.gcn3(g, h)\n",
        "        #h = torch.relu(h)\n",
        "        #h = self.gcn4(g, h)\n",
        "        #h = torch.relu(h)\n",
        "        h = self.gcn2(g, h)\n",
        "        h = self.softmax(h)\n",
        "        return h\n",
        "# The first layer transforms input features of size of 34 to a hidden size of 5.\n",
        "# The second layer transforms the hidden layer and produces output features of\n",
        "# size 2, corresponding to the two groups of the karate club.\n",
        "net = GCN(5, 5, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-iz_b1j7Blv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define one sample ground truth of the edge states\n",
        "import torch\n",
        "inputs_tensor = torch.from_numpy(inputs)\n",
        "labeled_nodes = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # only the instructor and the president nodes are labeled\n",
        "labels = torch.tensor([3, 0, 3, 3, 0, 0, 2, 0, 1, 3])  # their labels are different"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWrtNIbP7dEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#One example of training and validation\n",
        "#Training uses 5 scene graphs and validation uses 1 scene graph\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
        "all_preds = []\n",
        "first_all_preds = []\n",
        "epochs = 20\n",
        "train = []\n",
        "validation = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    acc_loss = 0\n",
        "    preds = net(G, first_inputs_tensor.float())\n",
        "    # we only compute loss for labeled nodes\n",
        "    loss = nn.CrossEntropyLoss(reduce=False)\n",
        "    Loss = loss(preds[labeled_nodes], first_labels)\n",
        "    acc_loss += Loss.sum()\n",
        "    optimizer.zero_grad() # PyTorch accumulates gradients by default\n",
        "    Loss.sum().backward() \n",
        "    optimizer.step()\n",
        "\n",
        "    preds = net(G, second_inputs_tensor.float())\n",
        "    # we only compute loss for labeled nodes\n",
        "    Loss = loss(preds[labeled_nodes], second_labels)\n",
        "    acc_loss += Loss.sum()\n",
        "    optimizer.zero_grad() # PyTorch accumulates gradients by default\n",
        "    Loss.sum().backward() \n",
        "    optimizer.step()\n",
        "\n",
        "    preds = net(G, third_inputs_tensor.float())\n",
        "    # we only compute loss for labeled nodes\n",
        "    Loss = loss(preds[labeled_nodes], third_labels)\n",
        "    acc_loss += Loss.sum()\n",
        "\n",
        "    preds = net(G, forth_inputs_tensor.float())\n",
        "    # we only compute loss for labeled nodes\n",
        "    Loss = loss(preds[labeled_nodes], forth_labels)\n",
        "    acc_loss += Loss.sum()\n",
        "    optimizer.zero_grad() # PyTorch accumulates gradients by default\n",
        "    Loss.sum().backward() \n",
        "    optimizer.step()\n",
        "\n",
        "    preds = net(G, fifth_inputs_tensor.float())\n",
        "    # we only compute loss for labeled nodes\n",
        "    Loss = loss(preds[labeled_nodes], fifth_labels)\n",
        "    acc_loss += Loss.sum()\n",
        "\n",
        "    train.append(acc_loss / 5)\n",
        "\n",
        "    val_preds = net(G, inputs_tensor.float())\n",
        "    val_Loss = loss(val_preds[labeled_nodes], labels)\n",
        "    validation.append(val_Loss.sum())\n",
        "\n",
        "    print('Epoch %d | Loss: %.4f' % (epoch, Loss.sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkU9bm6v7tuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Draw the graph of loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train)\n",
        "plt.plot(validation)\n",
        "plt.title('2-Layer-4-Class')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsNckzGs71R-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Do the testing on the model\n",
        "preds = net(G, test_inputs_tensor.float())\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}